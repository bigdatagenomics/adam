#!/usr/bin/env bash

set -e -x -v

# make a tempdir for writing maven cruft to
ADAM_MVN_TMP_DIR=$(mktemp -d -t adamTestMvnXXXXXXX)

# add this tempdir to the poms...
find . -name pom.xml \
    -exec sed -i.bak \
    -e "s:sun.io.serialization.extendedDebugInfo=true:sun.io.serialization.extendedDebugInfo=true -Djava.io.tmpdir=${ADAM_MVN_TMP_DIR}:g" \
    {} \;
find . -name "*.bak" -exec rm -f {} \;

# variable declarations
export PATH=${JAVA_HOME}/bin/:${PATH}
export MAVEN_OPTS="-Xmx1536m -XX:MaxPermSize=1g -Dfile.encoding=utf-8"
DIR=$( cd $( dirname ${BASH_SOURCE[0]} ) && pwd )
PROJECT_ROOT=${DIR}/..
VERSION=$(grep "<version>" ${PROJECT_ROOT}/pom.xml  | head -2 | tail -1 | sed 's/ *<version>//g' | sed 's/<\/version>//g')

# is the hadoop version set?
if ! [[ ${HADOOP_VERSION} ]];
then
    echo "HADOOP_VERSION environment variable is not set."
    echo "Please set this variable before running."
    
    exit 1
fi

# is the spark version set?
if ! [[ ${SPARK_VERSION} ]];
then
    echo "SPARK_VERSION environment variable is not set."
    echo "Please set this variable before running."
    
    exit 1
fi

# this next line is supposed to fail
set +e

echo "Rewriting POM.xml files to Scala 2.10 and Spark 1 should error..."
./scripts/move_to_spark_1.sh
if [[ $? == 0 ]];
then
    echo "Running move_to_spark_1.sh when POMs are set up for Spark 1 should fail, but error code was 0 (success)."
    exit 1
fi

./scripts/move_to_scala_2.10.sh
if [[ $? == 0 ]];
then
    echo "Running move_to_scala_2.10.sh when POMs are set up for Scala 2.10 should fail, but error code was 0 (success)."
    exit 1
fi

set -e

# are we testing for spark 2.2.0? if so, we need to rewrite our poms first
if [ ${SPARK_VERSION} == 2.2.0 ];
then
    
    echo "Rewriting POM.xml files for Spark 2."
    ./scripts/move_to_spark_2.sh

    # shouldn't be able to move to spark 2 twice
    set +e
    ./scripts/move_to_spark_2.sh
    if [[ $? == 0 ]];
    then
        echo "We have already moved to Spark 2, so running move_to_spark_2.sh a second time should fail, but error code was 0 (success)."
        exit 1
    fi
    set -e
fi

# are we testing for scala 2.11? if so, we need to rewrite our poms to 2.11 first
if [ ${SCALAVER} == 2.11 ];
then
    echo "Rewriting POM.xml files for Scala 2.11."
    ./scripts/move_to_scala_2.11.sh

    # shouldn't be able to move to scala 2.11 twice
    set +e
    ./scripts/move_to_scala_2.11.sh
    if [[ $? == 0 ]];
    then
        echo "We have already moved to Scala 2.11, so running move_to_scala_2.11.sh a second time should fail, but error code was 0 (success)."
        exit 1
    fi
    set -e
fi

# print versions
echo "Testing ADAM version ${VERSION} on Spark ${SPARK_VERSION} and Hadoop ${HADOOP_VERSION}"

# first, build the sources, run the unit tests, and generate a coverage report
mvn clean \
    -Dhadoop.version=${HADOOP_VERSION} \
    -Dspark.version=${SPARK_VERSION} 
    
# if this is a pull request, we need to set the coveralls pr id
if [[ ! -z $ghprbPullId ]];
then
    COVERALLS_PRB_OPTION="-DpullRequest=${ghprbPullId}"
fi

# coveralls token should not be visible
set +x +v

if [[ -z ${COVERALLS_REPO_TOKEN} ]];
then
    echo "Coveralls token is not set. Exiting..."
    exit 1
fi

# if those pass, build the distribution package and the integration tests
mvn -U \
    test \
    -P coverage,coveralls  scoverage:report coveralls:report \
    -DrepoToken=${COVERALLS_REPO_TOKEN} ${COVERALLS_PRB_OPTION}

# make verbose again
set -x -v

# if those pass, build the distribution package
mvn -U \
    -P distribution \
    package \
    -DskipTests \
    -Dhadoop.version=${HADOOP_VERSION} \
    -Dspark.version=${SPARK_VERSION} \
    -DargLine=${ADAM_MVN_TMP_DIR}

# make sure that the distribution package contains an assembly jar
# if no assembly jar is found, this will exit with code 1 and fail the build
tar tzvf adam-distribution/target/adam-distribution*-bin.tar.gz | \
    grep adam-assembly | \
    grep jar | \
    grep -v -e sources -e javadoc 

# we are done with maven, so clean up the maven temp dir
find ${ADAM_MVN_TMP_DIR}
rm -rf ${ADAM_MVN_TMP_DIR}

# and move our poms back to their original values
# this will allow us to pass our porcelain test at the end
if [ ${SPARK_VERSION} == 2.2.0 ];
then
    
    echo "Rewriting POM.xml files back to Spark 1."
    ./scripts/move_to_spark_1.sh
fi
if [ ${SCALAVER} == 2.11 ];
then
    echo "Rewriting POM.xml files back to Scala 2.10."
    ./scripts/move_to_scala_2.10.sh
fi

find . -name pom.xml \
    -exec sed -i.bak \
    -e "s:sun.io.serialization.extendedDebugInfo=true -Djava.io.tmpdir=${ADAM_MVN_TMP_DIR}:sun.io.serialization.extendedDebugInfo=true:g" \
    {} \;
find . -name "*.bak" -exec rm -f {} \;

if test -n "$(git status --porcelain)"
then
    echo "Applying move_to_xyz script marred a pom.xml file."
    echo "Exiting..."
    exit 1
fi

# run integration tests
# prebuilt spark distributions are scala 2.10 for spark 1.x, scala 2.11 for spark 2.x
if [[ ( ${SPARK_VERSION} != 2.2.0 && ${SCALAVER} == 2.10 && ${HADOOP_VERSION} == 2.6.2) || ( ${SPARK_VERSION} == 2.2.0 && ${SCALAVER} == 2.11 ) ]];
then

    # we moved away from spark 2/scala 2.11 in our poms earlier,
    # so rewrite poms
    if [ ${SPARK_VERSION} == 2.2.0 ];
    then
        
        echo "Rewriting POM.xml files for Spark 2."
        ./scripts/move_to_spark_2.sh

        echo "Rewriting POM.xml files for Scala 2.11."
        ./scripts/move_to_scala_2.11.sh
    fi

    # make a temp directory
    ADAM_TMP_DIR=$(mktemp -d -t adamTestXXXXXXX)

    # Just to be paranoid.. use a directory internal to the ADAM_TMP_DIR
    ADAM_TMP_DIR=$ADAM_TMP_DIR/deleteMePleaseThisIsNoLongerNeeded
    mkdir $ADAM_TMP_DIR

    # set the TMPDIR envar, which is used by python to choose where to make temp directories
    export TMPDIR=${ADAM_TMP_DIR}

    pushd $PROJECT_ROOT

    # Copy the jar into our temp space for testing
    cp -r . $ADAM_TMP_DIR
    popd

    pushd $ADAM_TMP_DIR

    # create a conda environment for python build, if necessary
    uuid=$(uuidgen)
    conda create -q -n adam-build-${uuid} python=2.7 anaconda
    source activate adam-build-${uuid}

    # what hadoop version are we on? format string for downloading spark assembly
    if [[ $HADOOP_VERSION =~ ^2\.6 ]]; then
        HADOOP=hadoop2.6
    elif [[ $HADOOP_VERSION =~ ^2\.7 ]]; then
        HADOOP=hadoop2.7
    else
        echo "Unknown Hadoop version."
        exit 1
    fi

    # set spark artifact string for downloading assembly
    SPARK=spark-${SPARK_VERSION}
    
    # download prepackaged spark assembly
    wget -q http://d3kbcqa49mib13.cloudfront.net/${SPARK}-bin-${HADOOP}.tgz
    tar xzvf ${SPARK}-bin-${HADOOP}.tgz
    export SPARK_HOME=${ADAM_TMP_DIR}/${SPARK}-bin-${HADOOP}
    
    # set the path to the adam submit script
    ADAM=./bin/adam-submit

    # add pyspark to the python path
    PY4J_ZIP="$(ls -1 "${SPARK_HOME}/python/lib" | grep py4j)"
    export PYTHONPATH=${SPARK_HOME}/python:${SPARK_HOME}/python/lib/${PY4J_ZIP}:${PYTHONPATH}

    # put adam jar on the pyspark path
    ASSEMBLY_DIR="${ADAM_TMP_DIR}/adam-assembly/target"
    ASSEMBLY_JAR="$(ls -1 "$ASSEMBLY_DIR" | grep "^adam[0-9A-Za-z\_\.-]*\.jar$" | grep -v javadoc | grep -v sources || true)"
    export PYSPARK_SUBMIT_ARGS="--jars ${ASSEMBLY_DIR}/${ASSEMBLY_JAR} --driver-class-path ${ASSEMBLY_DIR}/${ASSEMBLY_JAR} pyspark-shell"

    # we only support SparkR on Spark 2.x
    if [ ${SPARK_VERSION} == 2.2.0 ];
    then

	# make a directory to install SparkR into, and set the R user libs path
	export R_LIBS_USER=${SPARK_HOME}/local_R_libs
	mkdir -p ${R_LIBS_USER}
	R CMD INSTALL \
	    -l ${R_LIBS_USER} \
	    ${SPARK_HOME}/R/lib/SparkR/

	export SPARKR_SUBMIT_ARGS="--jars ${ASSEMBLY_DIR}/${ASSEMBLY_JAR} --driver-class-path ${ASSEMBLY_DIR}/${ASSEMBLY_JAR} sparkr-shell"
    
        # we can run the python build, now that we have a spark executable
	mvn -U \
            -P python,r \
            package \
            -DskipTests \
            -Dhadoop.version=${HADOOP_VERSION} \
            -Dspark.version=${SPARK_VERSION}

    else
        # we can run the python build, now that we have a spark executable
	mvn -U \
            -P python \
            package \
            -DskipTests \
            -Dhadoop.version=${HADOOP_VERSION} \
            -Dspark.version=${SPARK_VERSION}
    fi

    # copy python targets back
    cp -r adam-python/target ${PROJECT_ROOT}/adam-python/

    # deactivate and remove the conda env
    source deactivate
    conda remove -n adam-build-${uuid} --all

    # define filenames
    BAM=mouse_chrM.bam
    READS=${BAM}.reads.adam
    SORTED_READS=${BAM}.reads.sorted.adam
    FRAGMENTS=${BAM}.fragments.adam
    
    # fetch our input dataset
    echo "Fetching BAM file"
    rm -rf ${BAM}
    wget -q https://s3.amazonaws.com/bdgenomics-test/${BAM}

    # once fetched, convert BAM to ADAM
    echo "Converting BAM to ADAM read format"
    rm -rf ${READS}
    ${ADAM} transformAlignments ${BAM} ${READS}

    # then, sort the BAM
    echo "Converting BAM to ADAM read format with sorting"
    rm -rf ${SORTED_READS}
    ${ADAM} transformAlignments -sort_reads ${READS} ${SORTED_READS}

    # convert the reads to fragments to re-pair the reads
    echo "Converting read file to fragments"
    rm -rf ${FRAGMENTS}
    ${ADAM} transformFragments -load_as_reads ${READS} ${FRAGMENTS}

    # test that printing works
    echo "Printing reads and fragments"
    ${ADAM} print ${READS} 1>/dev/null 2>/dev/null
    ${ADAM} print ${FRAGMENTS} 1>/dev/null 2>/dev/null

    # run flagstat to verify that flagstat runs OK
    echo "Printing read statistics"
    ${ADAM} flagstat -print_metrics ${READS}
    rm -rf ${ADAM_TMP_DIR}
    popd

    # we rewrote our poms to spark 2/scala 2.11 earlier, so rewrite now
    if [ ${SPARK_VERSION} == 2.2.0 ];
    then
        
        echo "Reverting POM.xml file changes for Spark 2."
        ./scripts/move_to_spark_1.sh

        echo "Reverting POM.xml file changes for Scala 2.11."
        ./scripts/move_to_scala_2.10.sh
    fi
    
    # test that the source is formatted correctly
    # we had modified the poms to add a temp dir, so back out that modification first
    pushd ${PROJECT_ROOT}
    ./scripts/format-source
    if test -n "$(git status --porcelain)"
    then
        echo "Please run './scripts/format-source'"
        exit 1
    fi
    popd    
fi

echo
echo "All the tests passed"
echo
