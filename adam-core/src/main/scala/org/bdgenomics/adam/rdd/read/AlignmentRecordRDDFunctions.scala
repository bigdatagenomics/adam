/**
 * Licensed to Big Data Genomics (BDG) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The BDG licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.bdgenomics.adam.rdd.read

import java.io.{ OutputStream, StringWriter }
import htsjdk.samtools.{ SAMFileHeader, SAMTextHeaderCodec, SAMTextWriter, ValidationStringency }
import org.apache.avro.Schema
import org.apache.avro.file.DataFileWriter
import org.apache.avro.specific.{ SpecificDatumWriter, SpecificRecordBase }
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.{ FileSystem, FileUtil, Path }
import org.apache.hadoop.io.LongWritable
import org.apache.spark.SparkContext
import org.apache.spark.broadcast.Broadcast
import org.apache.spark.rdd.MetricsContext._
import org.apache.spark.rdd.{ PartitionPruningRDD, RDD }
import org.apache.spark.storage.StorageLevel
import org.bdgenomics.adam.algorithms.consensus.{ ConsensusGenerator, ConsensusGeneratorFromReads }
import org.bdgenomics.adam.converters.AlignmentRecordConverter
import org.bdgenomics.adam.instrumentation.Timers._
import org.bdgenomics.adam.models._
import org.bdgenomics.adam.rdd.ADAMContext._
import org.bdgenomics.adam.rdd.read.realignment.RealignIndels
import org.bdgenomics.adam.rdd.read.recalibration.BaseQualityRecalibration
import org.bdgenomics.adam.rdd.{ ADAMSaveAnyArgs, ADAMSequenceDictionaryRDDAggregator }
import org.bdgenomics.adam.rich.RichAlignmentRecord
import org.bdgenomics.adam.util.MapTools
import org.bdgenomics.formats.avro._
import org.seqdoop.hadoop_bam.SAMRecordWritable
import scala.reflect.ClassTag

import scala.language.implicitConversions

class AlignmentRecordRDDFunctions(rdd: RDD[AlignmentRecord])
    extends ADAMSequenceDictionaryRDDAggregator[AlignmentRecord](rdd) {

  /**
   * Calculates the subset of the RDD whose AlignmentRecords overlap the corresponding
   * query ReferenceRegion.  Equality of the reference sequence (to which these are aligned)
   * is tested by string equality of the names.  AlignmentRecords whose 'getReadMapped' method
   * return 'false' are ignored.
   *
   * The end of the record against the reference sequence is calculated from the cigar string
   * using the ADAMContext.referenceLengthFromCigar method.
   *
   * @param query The query region, only records which overlap this region are returned.
   * @return The subset of AlignmentRecords (corresponding to either primary or secondary alignments) that
   *         overlap the query region.
   */
  def filterByOverlappingRegion(query: ReferenceRegion): RDD[AlignmentRecord] = {
    def overlapsQuery(rec: AlignmentRecord): Boolean =
      rec.getReadMapped &&
        rec.getContig.getContigName == query.referenceName &&
        rec.getStart < query.end &&
        rec.getEnd > query.start
    rdd.filter(overlapsQuery)
  }

  private[rdd] def maybeSaveBam(
    args: ADAMSaveAnyArgs,
    sd: SequenceDictionary,
    rgd: RecordGroupDictionary,
    isSorted: Boolean = false): Boolean = {

    if (args.outputPath.endsWith(".sam")) {
      log.info("Saving data in SAM format")
      rdd.adamSAMSave(args.outputPath,
        sd,
        rgd,
        asSingleFile = args.asSingleFile,
        isSorted = isSorted)
      true
    } else if (args.outputPath.endsWith(".bam")) {
      log.info("Saving data in BAM format")
      rdd.adamSAMSave(args.outputPath,
        sd,
        rgd,
        asSam = false,
        asSingleFile = args.asSingleFile,
        isSorted = isSorted)
      true
    } else
      false
  }

  private[rdd] def maybeSaveFastq(args: ADAMSaveAnyArgs): Boolean = {
    if (args.outputPath.endsWith(".fq") || args.outputPath.endsWith(".fastq") ||
      args.outputPath.endsWith(".ifq")) {
      rdd.adamSaveAsFastq(args.outputPath, sort = args.sortFastqOutput)
      true
    } else
      false
  }

  /**
   * Saves Avro data to a Hadoop file system.
   *
   * This method uses a SparkContext to identify our underlying file system,
   * which we then save to.
   *
   * Frustratingly enough, although all records generated by the Avro IDL
   * compiler have a static SCHEMA$ field, this field does not belong to
   * the SpecificRecordBase abstract class, or the SpecificRecord interface.
   * As such, we must force the user to pass in the schema.
   *
   * @tparam T The type of the specific record we are saving.
   *
   * @param filename Path to save records to.
   * @param sc SparkContext used for identifying underlying file system.
   * @param schema Schema of records we are saving.
   * @param avro Seq of records we are saving.
   */
  private def saveAvro[T <: SpecificRecordBase](filename: String,
                                                sc: SparkContext,
                                                schema: Schema,
                                                avro: Seq[T])(implicit tTag: ClassTag[T]) {

    // get our current file system
    val fs = FileSystem.get(sc.hadoopConfiguration)

    // get an output stream
    val os = fs.create(new Path(filename))
      .asInstanceOf[OutputStream]

    // set up avro for writing
    val dw = new SpecificDatumWriter[T](schema)
    val fw = new DataFileWriter[T](dw)
    fw.create(schema, os)

    // write all our records
    avro.foreach(r => fw.append(r))

    // close the file
    fw.close()
    os.close()
  }

  /**
   * Saves AlignmentRecords as a directory of Parquet files.
   *
   * The RDD is written as a directory of Parquet files, with
   * Parquet configuration described by the input param args.
   * The provided sequence dictionary is written at args.outputPath.seqdict
   * while the provided record group dictionary is written at
   * args.outputPath.rgdict. These two files are written as Avro binary.
   *
   * @param args Save configuration arguments.
   * @param sd Sequence dictionary describing the contigs these reads are
   *   aligned to.
   * @param rgd Record group dictionary describing the record groups these
   *   reads are from.
   *
   * @see adamSave
   * @see adamAlignedRecordSave
   */
  def saveAsParquet(args: ADAMSaveAnyArgs,
                    sd: SequenceDictionary,
                    rgd: RecordGroupDictionary) = {
    // convert sequence dictionary and record group dictionaries to avro form
    val contigs = sd.records
      .map(SequenceRecord.toADAMContig)
      .toSeq
    val rgMetadata = rgd.recordGroups
      .map(_.toMetadata)

    // write the sequence dictionary and record group dictionary to disk
    saveAvro("%s.seqdict".format(args.outputPath),
      rdd.context,
      Contig.SCHEMA$,
      contigs)
    saveAvro("%s.rgdict".format(args.outputPath),
      rdd.context,
      RecordGroupMetadata.SCHEMA$,
      rgMetadata)

    // save rdd itself as parquet
    rdd.adamParquetSave(args)
  }

  /**
   * Saves AlignmentRecords as a directory of Parquet files or as SAM/BAM.
   *
   * This method infers the output format from the file extension. Filenames
   * ending in .sam/.bam are saved as SAM/BAM, and all other files are saved
   * as Parquet.
   *
   * @param args Save configuration arguments.
   * @param sd Sequence dictionary describing the contigs these reads are
   *   aligned to.
   * @param rgd Record group dictionary describing the record groups these
   *   reads are from.
   *
   * @see adamSave
   * @see adamSAMSave
   * @see saveAsParquet
   */
  def adamAlignedRecordSave(args: ADAMSaveAnyArgs,
                            sd: SequenceDictionary,
                            rgd: RecordGroupDictionary): Boolean = {
    maybeSaveBam(args, sd, rgd) || { saveAsParquet(args, sd, rgd); true }
  }

  /**
   * Saves AlignmentRecords as a directory of Parquet files or as SAM/BAM.
   *
   * This method infers the output format from the file extension. Filenames
   * ending in .sam/.bam are saved as SAM/BAM, and all other files are saved
   * as Parquet.
   *
   * @param args Save configuration arguments.
   * @param sd Sequence dictionary describing the contigs these reads are
   *   aligned to.
   * @param rgd Record group dictionary describing the record groups these
   *   reads are from.
   *
   * @see adamAlignedRecordSave
   * @see adamSAMSave
   * @see saveAsParquet
   * @see adamSaveAsFastq
   */
  def adamSave(
    args: ADAMSaveAnyArgs,
    sd: SequenceDictionary,
    rgd: RecordGroupDictionary,
    isSorted: Boolean = false): Boolean = {

    (maybeSaveBam(args, sd, rgd, isSorted) ||
      maybeSaveFastq(args) ||
      { saveAsParquet(args, sd, rgd); true })
  }

  /**
   * Converts an RDD into the SAM spec string it represents.
   *
   * This method converts an RDD of AlignmentRecords back to an RDD of
   * SAMRecordWritables and a SAMFileHeader, and then maps this RDD into a
   * string on the driver that represents this file in SAM.
   *
   * @param sd Sequence dictionary describing the contigs these reads are
   *   aligned to.
   * @param rgd Record group dictionary describing the record groups these
   *   reads are from.
   *
   * @return A string on the driver representing this RDD of reads in SAM format.
   *
   * @see adamConvertToSAM
   */
  def adamSAMString(sd: SequenceDictionary,
                    rgd: RecordGroupDictionary): String = {
    // convert the records
    val (convertRecords: RDD[SAMRecordWritable], header: SAMFileHeader) = rdd.adamConvertToSAM(sd, rgd)

    val records = convertRecords.coalesce(1, shuffle = true).collect()

    val samHeaderCodec = new SAMTextHeaderCodec
    samHeaderCodec.setValidationStringency(ValidationStringency.SILENT)

    val samStringWriter = new StringWriter()
    samHeaderCodec.encode(samStringWriter, header);

    val samWriter: SAMTextWriter = new SAMTextWriter(samStringWriter)
    //samWriter.writeHeader(stringHeaderWriter.toString)

    records.foreach(record => samWriter.writeAlignment(record.get))

    samStringWriter.toString
  }

  /**
   * Saves an RDD of ADAM read data into the SAM/BAM format.
   *
   * @param filePath Path to save files to.
   * @param sd A dictionary describing the contigs this file is aligned against.
   * @param rgd A dictionary describing the read groups in this file.
   * @param asSam Selects whether to save as SAM or BAM. The default value is true (save in SAM format).
   * @param asSingleFile If true, saves output as a single file.
   * @param isSorted If the output is sorted, this will modify the header.
   */
  def adamSAMSave(
    filePath: String,
    sd: SequenceDictionary,
    rgd: RecordGroupDictionary,
    asSam: Boolean = true,
    asSingleFile: Boolean = false,
    isSorted: Boolean = false) = SAMSave.time {

    // if the file is sorted, make sure the sequence dictionary is sorted
    val sdFinal = if (isSorted) {
      sd.sorted
    } else {
      sd
    }

    // convert the records
    val (convertRecords: RDD[SAMRecordWritable], header: SAMFileHeader) = rdd.adamConvertToSAM(sdFinal,
      rgd,
      isSorted)

    // add keys to our records
    val withKey = convertRecords.keyBy(v => new LongWritable(v.get.getAlignmentStart))

    val bcastHeader = rdd.context.broadcast(header)
    val mp = rdd.mapPartitionsWithIndex((idx, iter) => {
      log.info(s"Setting ${if (asSam) "SAM" else "BAM"} header for partition $idx")
      val header = bcastHeader.value
      synchronized {
        // perform map partition call to ensure that the SAM/BAM header is set on all
        // nodes in the cluster; see:
        // https://github.com/bigdatagenomics/adam/issues/353,
        // https://github.com/bigdatagenomics/adam/issues/676

        asSam match {
          case true =>
            ADAMSAMOutputFormat.clearHeader()
            ADAMSAMOutputFormat.addHeader(header)
            log.info(s"Set SAM header for partition $idx")
          case false =>
            ADAMBAMOutputFormat.clearHeader()
            ADAMBAMOutputFormat.addHeader(header)
            log.info(s"Set BAM header for partition $idx")
        }
      }
      Iterator[Int]()
    }).count()

    // force value check, ensure that computation happens
    if (mp != 0) {
      log.error("Had more than 0 elements after map partitions call to set VCF header across cluster.")
    }

    // attach header to output format
    asSam match {
      case true =>
        ADAMSAMOutputFormat.clearHeader()
        ADAMSAMOutputFormat.addHeader(header)
        log.info("Set SAM header on driver")
      case false =>
        ADAMBAMOutputFormat.clearHeader()
        ADAMBAMOutputFormat.addHeader(header)
        log.info("Set BAM header on driver")
    }

    // write file to disk
    val conf = rdd.context.hadoopConfiguration

    if (!asSingleFile) {
      asSam match {
        case true =>
          withKey.saveAsNewAPIHadoopFile(
            filePath,
            classOf[LongWritable],
            classOf[SAMRecordWritable],
            classOf[InstrumentedADAMSAMOutputFormat[LongWritable]],
            conf
          )
        case false =>
          withKey.saveAsNewAPIHadoopFile(
            filePath,
            classOf[LongWritable],
            classOf[SAMRecordWritable],
            classOf[InstrumentedADAMBAMOutputFormat[LongWritable]],
            conf
          )

      }
    } else {
      log.info(s"Writing single ${if (asSam) "SAM" else "BAM"} file (not Hadoop-style directory)")
      val (outputFormat, headerLessOutputFormat) = asSam match {
        case true =>
          (
            classOf[InstrumentedADAMSAMOutputFormat[LongWritable]],
            classOf[InstrumentedADAMSAMOutputFormatHeaderLess[LongWritable]]
          )

        case false =>
          (
            classOf[InstrumentedADAMBAMOutputFormat[LongWritable]],
            classOf[InstrumentedADAMBAMOutputFormatHeaderLess[LongWritable]]
          )
      }

      val headPath = filePath + "_head"
      val tailPath = filePath + "_tail"

      PartitionPruningRDD.create(withKey, i => { i == 0 }).saveAsNewAPIHadoopFile(
        headPath,
        classOf[LongWritable],
        classOf[SAMRecordWritable],
        outputFormat,
        conf
      )
      PartitionPruningRDD.create(withKey, i => { i != 0 }).saveAsNewAPIHadoopFile(
        tailPath,
        classOf[LongWritable],
        classOf[SAMRecordWritable],
        headerLessOutputFormat,
        conf
      )

      val fs = FileSystem.get(conf)

      fs.listStatus(headPath)
        .find(_.getPath.getName.startsWith("part-r"))
        .map(fileStatus => FileUtil.copy(fs, fileStatus.getPath, fs, tailPath + "/head", false, false, conf))
      fs.delete(headPath, true)

      FileUtil.copyMerge(fs, tailPath, fs, filePath, true, conf, null)
      fs.delete(tailPath, true)

    }

  }

  implicit def str2Path(str: String): Path = new Path(str)

  def getSequenceRecordsFromElement(elem: AlignmentRecord): Set[SequenceRecord] = {
    SequenceRecord.fromADAMRecord(elem).toSet
  }

  /**
   * Converts an RDD of ADAM read records into SAM records.
   *
   * @return Returns a SAM/BAM formatted RDD of reads, as well as the file header.
   */
  def adamConvertToSAM(sd: SequenceDictionary,
                       rgd: RecordGroupDictionary,
                       isSorted: Boolean = false): (RDD[SAMRecordWritable], SAMFileHeader) = ConvertToSAM.time {

    // create conversion object
    val adamRecordConverter = new AlignmentRecordConverter

    // create header
    val header = adamRecordConverter.createSAMHeader(sd, rgd)

    if (isSorted) {
      header.setSortOrder(SAMFileHeader.SortOrder.coordinate)
    }

    // broadcast for efficiency
    val hdrBcast = rdd.context.broadcast(SAMFileHeaderWritable(header))

    // map across RDD to perform conversion
    val convertedRDD: RDD[SAMRecordWritable] = rdd.map(r => {
      // must wrap record for serializability
      val srw = new SAMRecordWritable()
      srw.set(adamRecordConverter.convert(r, hdrBcast.value, rgd))
      srw
    })

    (convertedRDD, header)
  }

  /**
   * Cuts reads into _k_-mers, and then counts the number of occurrences of each _k_-mer.
   *
   * @param kmerLength The value of _k_ to use for cutting _k_-mers.
   * @return Returns an RDD containing k-mer/count pairs.
   *
   * @see adamCountQmers
   */
  def adamCountKmers(kmerLength: Int): RDD[(String, Long)] = {
    rdd.flatMap(r => {
      // cut each read into k-mers, and attach a count of 1L
      r.getSequence
        .sliding(kmerLength)
        .map(k => (k, 1L))
    }).reduceByKey((k1: Long, k2: Long) => k1 + k2)
  }

  def adamSortReadsByReferencePosition(): RDD[AlignmentRecord] = SortReads.time {
    log.info("Sorting reads by reference position")

    // NOTE: In order to keep unmapped reads from swamping a single partition
    // we sort the unmapped reads by read name. We prefix with tildes ("~";
    // ASCII 126) to ensure that the read name is lexicographically "after" the
    // contig names.
    rdd.keyBy(r => {
      if (r.getReadMapped) {
        ReferencePosition(r)
      } else {
        ReferencePosition(s"~~~${r.getReadName}", 0)
      }
    }).sortByKey().map(_._2)
  }

  /**
   * Marks reads as possible fragment duplicates.
   *
   * @param rgd A dictionary mapping read groups to sequencing libraries. This
   *   is used when deduping reads, as we only dedupe reads that are from the
   *   same original library.
   * @return A new RDD where reads have the duplicate read flag set. Duplicate
   *   reads are NOT filtered out.
   */
  def adamMarkDuplicates(rgd: RecordGroupDictionary): RDD[AlignmentRecord] = MarkDuplicatesInDriver.time {
    MarkDuplicates(rdd, rgd)
  }

  /**
   * Runs base quality score recalibration on a set of reads. Uses a table of
   * known SNPs to mask true variation during the recalibration process.
   *
   * @param knownSnps A table of known SNPs to mask valid variants.
   * @param observationDumpFile An optional local path to dump recalibration
   *                            observations to.
   * @return Returns an RDD of recalibrated reads.
   */
  def adamBQSR(
    knownSnps: Broadcast[SnpTable],
    observationDumpFile: Option[String] = None,
    validationStringency: ValidationStringency = ValidationStringency.LENIENT): RDD[AlignmentRecord] = BQSRInDriver.time {
    BaseQualityRecalibration(rdd, knownSnps, observationDumpFile, validationStringency)
  }

  /**
   * Realigns indels using a concensus-based heuristic.
   *
   * @see RealignIndels
   *
   * @param isSorted If the input data is sorted, setting this parameter to true avoids a second sort.
   * @param maxIndelSize The size of the largest indel to use for realignment.
   * @param maxConsensusNumber The maximum number of consensus sequences to realign against per
   * target region.
   * @param lodThreshold Log-odds threhold to use when realigning; realignments are only finalized
   * if the log-odds threshold is exceeded.
   * @param maxTargetSize The maximum width of a single target region for realignment.
   *
   * @return Returns an RDD of mapped reads which have been realigned.
   */
  def adamRealignIndels(
    consensusModel: ConsensusGenerator = new ConsensusGeneratorFromReads,
    isSorted: Boolean = false,
    maxIndelSize: Int = 500,
    maxConsensusNumber: Int = 30,
    lodThreshold: Double = 5.0,
    maxTargetSize: Int = 3000): RDD[AlignmentRecord] = RealignIndelsInDriver.time {
    RealignIndels(rdd, consensusModel, isSorted, maxIndelSize, maxConsensusNumber, lodThreshold)
  }

  // Returns a tuple of (failedQualityMetrics, passedQualityMetrics)
  def adamFlagStat(): (FlagStatMetrics, FlagStatMetrics) = {
    FlagStat(rdd)
  }

  /**
   * Groups all reads by record group and read name
   * @return SingleReadBuckets with primary, secondary and unmapped reads
   */
  def adamSingleReadBuckets(): RDD[SingleReadBucket] = {
    SingleReadBucket(rdd)
  }

  /**
   * Converts a set of records into an RDD containing the pairs of all unique tagStrings
   * within the records, along with the count (number of records) which have that particular
   * attribute.
   *
   * @return An RDD of attribute name / count pairs.
   */
  def adamCharacterizeTags(): RDD[(String, Long)] = {
    rdd.flatMap(RichAlignmentRecord(_).tags.map(attr => (attr.tag, 1L))).reduceByKey(_ + _)
  }

  /**
   * Calculates the set of unique attribute <i>values</i> that occur for the given
   * tag, and the number of time each value occurs.
   *
   * @param tag The name of the optional field whose values are to be counted.
   * @return A Map whose keys are the values of the tag, and whose values are the number of time each tag-value occurs.
   */
  def adamCharacterizeTagValues(tag: String): Map[Any, Long] = {
    adamFilterRecordsWithTag(tag).flatMap(RichAlignmentRecord(_).tags.find(_.tag == tag)).map(
      attr => Map(attr.value -> 1L)
    ).reduce {
        (map1: Map[Any, Long], map2: Map[Any, Long]) =>
          MapTools.add(map1, map2)
      }
  }

  /**
   * Returns the subset of the ADAMRecords which have an attribute with the given name.
   * @param tagName The name of the attribute to filter on (should be length 2)
   * @return An RDD[Read] containing the subset of records with a tag that matches the given name.
   */
  def adamFilterRecordsWithTag(tagName: String): RDD[AlignmentRecord] = {
    assert(
      tagName.length == 2,
      "withAttribute takes a tagName argument of length 2; tagName=\"%s\"".format(tagName)
    )
    rdd.filter(RichAlignmentRecord(_).tags.exists(_.tag == tagName))
  }

  /**
   * Saves these AlignmentRecords to two FASTQ files: one for the first mate in each pair, and the other for the second.
   *
   * @param fileName1 Path at which to save a FASTQ file containing the first mate of each pair.
   * @param fileName2 Path at which to save a FASTQ file containing the second mate of each pair.
   * @param validationStringency Iff strict, throw an exception if any read in this RDD is not accompanied by its mate.
   */
  def adamSaveAsPairedFastq(
    fileName1: String,
    fileName2: String,
    outputOriginalBaseQualities: Boolean = false,
    validationStringency: ValidationStringency = ValidationStringency.LENIENT,
    persistLevel: Option[StorageLevel] = None): Unit = {
    def maybePersist[T](r: RDD[T]): Unit = {
      persistLevel.foreach(r.persist(_))
    }
    def maybeUnpersist[T](r: RDD[T]): Unit = {
      persistLevel.foreach(_ => r.unpersist())
    }

    maybePersist(rdd)
    val numRecords = rdd.count()

    val readsByID: RDD[(String, Iterable[AlignmentRecord])] =
      rdd.groupBy(record => {
        if (!AlignmentRecordConverter.readNameHasPairedSuffix(record))
          record.getReadName
        else
          record.getReadName.dropRight(2)
      })

    validationStringency match {
      case ValidationStringency.STRICT | ValidationStringency.LENIENT =>
        val readIDsWithCounts: RDD[(String, Int)] = readsByID.mapValues(_.size)
        val unpairedReadIDsWithCounts: RDD[(String, Int)] = readIDsWithCounts.filter(_._2 != 2)
        maybePersist(unpairedReadIDsWithCounts)

        val numUnpairedReadIDsWithCounts: Long = unpairedReadIDsWithCounts.count()
        if (numUnpairedReadIDsWithCounts != 0) {
          val readNameOccurrencesMap: collection.Map[Int, Long] = unpairedReadIDsWithCounts.map(_._2).countByValue()

          val msg =
            List(
              s"Found $numUnpairedReadIDsWithCounts read names that don't occur exactly twice:",

              readNameOccurrencesMap.take(100).map({
                case (numOccurrences, numReadNames) => s"${numOccurrences}x:\t$numReadNames"
              }).mkString("\t", "\n\t", if (readNameOccurrencesMap.size > 100) "\n\t…" else ""),
              "",

              "Samples:",
              unpairedReadIDsWithCounts
                .take(100)
                .map(_._1)
                .mkString("\t", "\n\t", if (numUnpairedReadIDsWithCounts > 100) "\n\t…" else "")
            ).mkString("\n")

          if (validationStringency == ValidationStringency.STRICT)
            throw new IllegalArgumentException(msg)
          else if (validationStringency == ValidationStringency.LENIENT)
            logError(msg)
        }
      case ValidationStringency.SILENT =>
    }

    val pairedRecords: RDD[AlignmentRecord] = readsByID.filter(_._2.size == 2).map(_._2).flatMap(x => x)
    maybePersist(pairedRecords)
    val numPairedRecords = pairedRecords.count()

    maybeUnpersist(rdd.unpersist())

    val firstInPairRecords: RDD[AlignmentRecord] = pairedRecords.filter(_.getReadInFragment == 0)
    maybePersist(firstInPairRecords)
    val numFirstInPairRecords = firstInPairRecords.count()

    val secondInPairRecords: RDD[AlignmentRecord] = pairedRecords.filter(_.getReadInFragment == 1)
    maybePersist(secondInPairRecords)
    val numSecondInPairRecords = secondInPairRecords.count()

    maybeUnpersist(pairedRecords)

    log.info(
      "%d/%d records are properly paired: %d firsts, %d seconds".format(
        numPairedRecords,
        numRecords,
        numFirstInPairRecords,
        numSecondInPairRecords
      )
    )

    assert(
      numFirstInPairRecords == numSecondInPairRecords,
      "Different numbers of first- and second-reads: %d vs. %d".format(numFirstInPairRecords, numSecondInPairRecords)
    )

    val arc = new AlignmentRecordConverter

    firstInPairRecords
      .sortBy(_.getReadName)
      .map(record => arc.convertToFastq(record, maybeAddSuffix = true, outputOriginalBaseQualities = outputOriginalBaseQualities))
      .saveAsTextFile(fileName1)

    secondInPairRecords
      .sortBy(_.getReadName)
      .map(record => arc.convertToFastq(record, maybeAddSuffix = true, outputOriginalBaseQualities = outputOriginalBaseQualities))
      .saveAsTextFile(fileName2)

    maybeUnpersist(firstInPairRecords)
    maybeUnpersist(secondInPairRecords)
  }

  /**
   * Saves reads in FASTQ format.
   *
   * @param fileName Path to save files at.
   * @param outputOriginalBaseQualities Output the original base qualities (OQ) if available as opposed to those from BQSR
   * @param sort Whether to sort the FASTQ files by read name or not. Defaults
   *             to false. Sorting the output will recover pair order, if desired.
   */
  def adamSaveAsFastq(
    fileName: String,
    fileName2Opt: Option[String] = None,
    outputOriginalBaseQualities: Boolean = false,
    sort: Boolean = false,
    validationStringency: ValidationStringency = ValidationStringency.LENIENT,
    persistLevel: Option[StorageLevel] = None) {
    log.info("Saving data in FASTQ format.")
    fileName2Opt match {
      case Some(fileName2) =>
        adamSaveAsPairedFastq(
          fileName,
          fileName2,
          outputOriginalBaseQualities = outputOriginalBaseQualities,
          validationStringency = validationStringency,
          persistLevel = persistLevel
        )
      case _ =>
        val arc = new AlignmentRecordConverter

        // sort the rdd if desired
        val outputRdd = if (sort || fileName2Opt.isDefined) {
          rdd.sortBy(_.getReadName)
        } else {
          rdd
        }

        // convert the rdd and save as a text file
        outputRdd
          .map(record => arc.convertToFastq(record, outputOriginalBaseQualities = outputOriginalBaseQualities))
          .saveAsTextFile(fileName)
    }
  }

  /**
   * Reassembles read pairs from two sets of unpaired reads. The assumption is that the two sets
   * were _originally_ paired together.
   *
   * @note The RDD that this is called on should be the RDD with the first read from the pair.
   *
   * @param secondPairRdd The rdd containing the second read from the pairs.
   * @param validationStringency How stringently to validate the reads.
   * @return Returns an RDD with the pair information recomputed.
   */
  def adamRePairReads(
    secondPairRdd: RDD[AlignmentRecord],
    validationStringency: ValidationStringency = ValidationStringency.LENIENT): RDD[AlignmentRecord] = {
    // cache rdds
    val firstPairRdd = rdd.cache()
    secondPairRdd.cache()

    val firstRDDKeyedByReadName = firstPairRdd.keyBy(_.getReadName.dropRight(2))
    val secondRDDKeyedByReadName = secondPairRdd.keyBy(_.getReadName.dropRight(2))

    // all paired end reads should have the same name, except for the last two
    // characters, which will be _1/_2
    val joinedRDD: RDD[(String, (AlignmentRecord, AlignmentRecord))] =
      if (validationStringency == ValidationStringency.STRICT) {
        firstRDDKeyedByReadName.cogroup(secondRDDKeyedByReadName).map {
          case (readName, (firstReads, secondReads)) =>
            (firstReads.toList, secondReads.toList) match {
              case (firstRead :: Nil, secondRead :: Nil) =>
                (readName, (firstRead, secondRead))
              case _ =>
                throw new Exception(
                  "Expected %d first reads and %d second reads for name %s; expected exactly one of each:\n%s\n%s".format(
                    firstReads.size,
                    secondReads.size,
                    readName,
                    firstReads.map(_.getReadName).mkString("\t", "\n\t", ""),
                    secondReads.map(_.getReadName).mkString("\t", "\n\t", "")
                  )
                )
            }
        }

      } else {
        firstRDDKeyedByReadName.join(secondRDDKeyedByReadName)
      }

    val finalRdd = joinedRDD
      .flatMap(kv => Seq(
        AlignmentRecord.newBuilder(kv._2._1)
          .setReadPaired(true)
          .setProperPair(true)
          .setReadInFragment(0)
          .build(),
        AlignmentRecord.newBuilder(kv._2._2)
          .setReadPaired(true)
          .setProperPair(true)
          .setReadInFragment(1)
          .build()
      ))

    // uncache temp rdds
    firstPairRdd.unpersist()
    secondPairRdd.unpersist()

    // return
    finalRdd
  }

  def toFragments: RDD[Fragment] = {
    adamSingleReadBuckets.map(_.toFragment)
  }
}
